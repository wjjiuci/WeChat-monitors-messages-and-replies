import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"  # ä½¿ç”¨ HF é•œåƒåŠ é€Ÿä¸‹è½½ï¼ˆå›½å†…æ¨èï¼‰
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import torch
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer
)
from datasets import Dataset
from snownlp import SnowNLP


# === 1. æƒ…æ„Ÿåˆ†æï¼šSnowNLP æ‰“æ ‡ ===
def add_sentiment_analysis(texts):
    """
    ä½¿ç”¨ SnowNLP å¯¹æ–‡æœ¬æ‰“æƒ…æ„Ÿæ ‡ç­¾ï¼ˆ0:è´Ÿ, 1:ä¸­, 2:æ­£ï¼‰
    """
    labels = []
    for text in texts:
        if not text or not text.strip():
            labels.append(1)  # ç©ºæ¶ˆæ¯è§†ä¸ºä¸­æ€§
            continue
        try:
            score = SnowNLP(text).sentiments  # 0~1 çš„æ­£é¢æ¦‚ç‡
        except Exception:
            score = 0.5
        if score < 0.4:
            label = 0
        elif score > 0.6:
            label = 2
        else:
            label = 1
        labels.append(label)
    return labels


# === 2. è¯»å–èŠå¤©è®°å½• ===
def read_chat_data(file_path):
    """
    ä»æ–‡ä»¶è¯»å–æ¶ˆæ¯ï¼Œæ ¼å¼ï¼šName:æ¶ˆæ¯å†…å®¹
    è¿”å›çº¯æ–‡æœ¬åˆ—è¡¨
    """
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line and ':' in line:
                content = line.split(':', 1)[1].strip()
                if content:
                    data.append(content)
    return data


# === 3. åˆ›å»ºæ•°æ®é›† ===
def create_dataset(texts, labels):
    return Dataset.from_dict({
        'text': texts,
        'sentiment': labels
    })


# === 4. æ•°æ®é¢„å¤„ç† ===
def preprocess_function(examples, tokenizer):
    return tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=128,
        return_tensors="pt"
    )


# === 5. ä¸»è®­ç»ƒæµç¨‹ ===
def main():
    name = "xxxå¥½å¤§å„¿"  #  ä¿®æ”¹ä¸ºä½ è‡ªå·±çš„è”ç³»äººå¤‡æ³¨å

    # å…³é”®ï¼šè·å– train.py æ‰€åœ¨ç›®å½•ï¼ˆä¸æ˜¯å½“å‰å·¥ä½œç›®å½•ï¼ï¼‰
    current_dir = os.path.dirname(os.path.abspath(__file__))
    file_path = os.path.join(current_dir, f"{name}æ‰€æœ‰èŠå¤©è®°å½•.txt")

    # æ£€æŸ¥èŠå¤©è®°å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(file_path):
        print(f"èŠå¤©è®°å½•æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        print("è¯·å…ˆè·å–èŠå¤©è®°å½•ï¼ˆä¾‹å¦‚ä½¿ç”¨ wxauto è·å–ï¼‰")
        return

    print(" æ­£åœ¨è¯»å–èŠå¤©è®°å½•...")
    chat_data = read_chat_data(file_path)
    if not chat_data:
        print(" èŠå¤©è®°å½•ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®ï¼ˆéœ€åŒ…å« ':' åˆ†éš”çš„è¡Œï¼‰")
        return

    print(f" æˆåŠŸè¯»å– {len(chat_data)} æ¡æ¶ˆæ¯ã€‚")

    print(" æ­£åœ¨è¿›è¡Œä¸­æ–‡æƒ…æ„Ÿåˆ†æï¼ˆä½¿ç”¨ SnowNLPï¼‰...")
    sentiment_labels = add_sentiment_analysis(chat_data)

    print(" æ ‡ç­¾åˆ†å¸ƒç»Ÿè®¡:")
    neg = sum(1 for x in sentiment_labels if x == 0)
    neu = sum(1 for x in sentiment_labels if x == 1)
    pos = sum(1 for x in sentiment_labels if x == 2)
    print(f"   - è´Ÿå‘: {neg}")
    print(f"   - ä¸­æ€§: {neu}")
    print(f"   - æ­£å‘: {pos}")

    print("ï¸ åˆ›å»ºå¹¶åˆ’åˆ†æ•°æ®é›†ï¼ˆ8:2ï¼‰...")
    dataset = create_dataset(chat_data, sentiment_labels)
    dataset = dataset.train_test_split(test_size=0.2)

    print(" åŠ è½½ BERT ä¸­æ–‡æ¨¡å‹ä¸åˆ†è¯å™¨...")
    model_name = "bert-base-chinese"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name,
        num_labels=3
    )

    print("ï¸ é¢„å¤„ç†æ•°æ®ï¼ˆåˆ†è¯ã€å¡«å……ã€æˆªæ–­ï¼‰...")
    tokenized_dataset = dataset.map(
        lambda examples: preprocess_function(examples, tokenizer),
        batched=True,
        remove_columns=["text"]
    )

    #  é‡å‘½ååˆ—ï¼š'sentiment' â†’ 'labels'ï¼ˆTrainer è¦æ±‚ï¼‰
    tokenized_dataset = tokenized_dataset.rename_column("sentiment", "labels")

    # è®¾ç½® PyTorch æ ¼å¼
    tokenized_dataset.set_format(
        type='torch',
        columns=['input_ids', 'attention_mask', 'labels']
    )

    print(" å¼€å§‹è®­ç»ƒï¼ˆå…¼å®¹æ—§ç‰ˆ transformersï¼‰...")
    #  ä½¿ç”¨å…¼å®¹æ—§ç‰ˆæœ¬çš„ TrainingArgumentsï¼ˆæ”¯æŒ transformers >= 3.0ï¼‰
    training_args = TrainingArguments(
        output_dir=os.path.join(current_dir, './results'),
        num_train_epochs=3,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=64,
        warmup_steps=500,
        weight_decay=0.01,
        logging_dir=os.path.join(current_dir, './logs'),
        logging_steps=10,
        save_strategy="steps",      # æ—§ç‰ˆåªæ”¯æŒ "steps" æˆ– "no"
        save_steps=100,             # æ¯100æ­¥ä¿å­˜ä¸€æ¬¡ checkpoint
        report_to="none"            # ä¸ä¸ŠæŠ¥åˆ° wandb ç­‰
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset["train"],
        eval_dataset=tokenized_dataset["test"],
        tokenizer=tokenizer
    )

    trainer.train()

    print(" ä¿å­˜å¾®è°ƒæ¨¡å‹å’Œåˆ†è¯å™¨...")
    model_save_path = os.path.join(current_dir, f"{name}_finetuned_model")
    model.save_pretrained(model_save_path)
    tokenizer.save_pretrained(model_save_path)

    print(f" æ¨¡å‹å·²æˆåŠŸä¿å­˜è‡³: {model_save_path}")
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼ç°åœ¨å¯åœ¨åŒç›®å½•ä¸‹çš„æ¨ç†è„šæœ¬ä¸­åŠ è½½æ­¤æ¨¡å‹ã€‚")


if __name__ == "__main__":
    main()




#